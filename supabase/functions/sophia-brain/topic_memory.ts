/**
 * Topic Memory System ‚Äî M√©moire th√©matique vivante
 *
 * Ce module g√®re des SYNTH√àSES √âVOLUTIVES par topic, avec des mots-cl√©s
 * vectoris√©s qui pointent vers ces synth√®ses.
 *
 * Flux :
 * 1. Le Watcher analyse la conversation ‚Üí extrait des topics + infos
 * 2. Pour chaque topic : on cherche si un topic similaire existe d√©j√†
 * 3. Si oui : on enrichit la synth√®se existante
 * 4. Si non : on cr√©e un nouveau topic
 * 5. On ajoute les mots-cl√©s (aliases) qui pointent vers le topic
 *
 * Retrieval :
 * - Le message user est vectoris√©
 * - On cherche par similarit√© dans les keywords ‚Üí retourne les synth√®ses
 * - On cherche aussi par similarit√© directe sur les synth√®ses (backup)
 * - Les topics pertinents sont inject√©s dans le contexte du prompt
 */

import { SupabaseClient } from "jsr:@supabase/supabase-js@2"
import { generateWithGemini, generateEmbedding } from "../_shared/gemini.ts"

type TopicEnrichmentSource = "chat" | "onboarding" | "bilan" | "module" | "plan"

// ============================================================================
// Types
// ============================================================================

/** Topic extrait d'une conversation par le LLM */
export interface ExtractedTopic {
  /** Slug canonique (ex: "cannabis_arret", "soeur_tania") */
  slug: string
  /** Titre lisible (ex: "Cannabis / Arr√™t", "S≈ìur (Tania)") */
  title: string
  /** Nouvelles informations √† int√©grer dans la synth√®se */
  new_information: string
  /** Mots-cl√©s / aliases associ√©s (ex: ["cannabis", "weed", "joint", "fumer"]) */
  keywords: string[]
  /** Domaine s√©mantique (ex: "sant√©", "famille", "travail", "loisirs") */
  domain?: string
}

/** Topic tel qu'il existe en base */
export interface TopicMemory {
  id: string
  user_id: string
  slug: string
  title: string
  synthesis: string
  status: string
  mention_count: number
  enrichment_count: number
  first_mentioned_at: string
  last_enriched_at: string | null
  last_retrieved_at: string | null
  metadata: Record<string, unknown>
}

/** R√©sultat de la recherche de topics par similarit√© */
export interface TopicSearchResult {
  topic_id: string
  slug: string
  title: string
  synthesis: string
  keyword_matched?: string
  keyword_similarity?: number
  synthesis_similarity?: number
  mention_count: number
  last_enriched_at: string | null
  metadata: Record<string, unknown>
}

// ============================================================================
// 1. EXTRACTION ‚Äî Analyser la conversation pour d√©tecter des topics
// ============================================================================

/**
 * Extrait les topics d'un transcript de conversation.
 * Appel√© par le Watcher apr√®s chaque batch de messages.
 */
export async function extractTopicsFromTranscript(opts: {
  transcript: string
  existingTopicSlugs: string[]
  currentContext?: string
  meta?: { requestId?: string; model?: string; forceRealAi?: boolean }
}): Promise<ExtractedTopic[]> {
  const { transcript, existingTopicSlugs, currentContext, meta } = opts

  const existingTopicsHint = existingTopicSlugs.length > 0
    ? `\nTOPICS D√âJ√Ä CONNUS pour cet utilisateur : ${existingTopicSlugs.join(", ")}\nSi une information enrichit un topic existant, utilise le M√äME slug.\n`
    : ""

  const prompt = `
Tu es un analyseur de m√©moire th√©matique pour un coach IA.
Tu lis un bloc de conversation et tu extrais les TOPICS significatifs.

Un TOPIC = un sujet de vie r√©current ou important pour l'utilisateur.
Exemples de topics : une personne (s≈ìur, patron), une habitude (sport, cannabis), un objectif (changer de job), une √©motion r√©currente (anxi√©t√© sociale), un √©v√©nement (d√©m√©nagement).

INPUTS :
- Conversation r√©cente (ci-dessous)
- Contexte pr√©c√©dent : "${currentContext ?? "Aucun"}"
${existingTopicsHint}

TES R√àGLES :
1. Ne cr√©e un topic QUE s'il y a de l'information SUBSTANTIELLE (pas juste une mention passag√®re).
2. Pour les PERSONNES mentionn√©es : le slug doit inclure le lien ET le pr√©nom s'il est connu (ex: "soeur_tania", "patron_marc").
3. Les keywords doivent inclure TOUTES les fa√ßons dont l'utilisateur pourrait r√©f√©rencer ce topic :
   - Synonymes ("cannabis", "weed", "joint", "shit", "fumer")
   - Liens familiaux ("ma s≈ìur", "tania", "ma frangine")
   - Termes connexes importants ("arr√™ter de fumer", "sevrage", "addiction")
4. Le champ "new_information" doit contenir un r√©sum√© dense de ce qui a √©t√© dit dans CE bloc.
5. Le champ "domain" aide √† connecter des topics entre eux (ex: "alimentation" et "allergie" sont dans le domaine "sant√©").
6. Maximum 4 topics par batch (garde seulement les plus significatifs).
7. Si RIEN de significatif n'a √©t√© dit (small talk, "ok", "merci"), retourne un tableau vide.

SORTIE JSON ATTENDUE :
{
  "topics": [
    {
      "slug": "cannabis_arret",
      "title": "Cannabis / Arr√™t",
      "new_information": "L'utilisateur dit avoir r√©duit sa consommation de moiti√© depuis 2 semaines. Il ressent des insomnies mais se sent plus lucide le matin.",
      "keywords": ["cannabis", "weed", "joint", "fumer", "arr√™ter de fumer", "sevrage"],
      "domain": "sant√©"
    }
  ]
}
  `.trim()

  try {
    const raw = await generateWithGemini(prompt, transcript, 0.2, true, [], "json", {
      requestId: meta?.requestId,
      model: meta?.model ?? "gemini-2.5-flash",
      source: "sophia-brain:topic_extraction",
      forceRealAi: meta?.forceRealAi,
    })

    const parsed = JSON.parse(String(raw ?? "{}"))
    const topics = Array.isArray(parsed?.topics) ? parsed.topics : []

    return topics
      .filter((t: any) => t?.slug && t?.title && t?.new_information)
      .slice(0, 4)
      .map((t: any) => ({
        slug: slugify(String(t.slug)),
        title: String(t.title).trim(),
        new_information: String(t.new_information).trim(),
        keywords: Array.isArray(t.keywords)
          ? t.keywords.map((k: any) => String(k).trim().toLowerCase()).filter(Boolean)
          : [],
        domain: t.domain ? String(t.domain).trim().toLowerCase() : undefined,
      }))
  } catch (e) {
    console.error("[TopicMemory] Failed to extract topics:", e)
    return []
  }
}

// ============================================================================
// 2. MATCHING ‚Äî Trouver les topics existants similaires
// ============================================================================

/**
 * Cherche les topics existants qui matchent un nouveau topic extrait.
 * Utilise √† la fois le slug exact ET la similarit√© s√©mantique des keywords.
 */
export async function findMatchingTopic(opts: {
  supabase: SupabaseClient
  userId: string
  extractedTopic: ExtractedTopic
}): Promise<TopicMemory | null> {
  const { supabase, userId, extractedTopic } = opts

  // 1. Chercher par slug exact (match direct)
  const { data: exactMatch } = await supabase
    .from("user_topic_memories")
    .select("*")
    .eq("user_id", userId)
    .eq("slug", extractedTopic.slug)
    .eq("status", "active")
    .maybeSingle()

  if (exactMatch) return exactMatch as TopicMemory

  // 2. Chercher par similarit√© s√©mantique sur les keywords
  if (extractedTopic.keywords.length > 0) {
    // On vectorise le titre + le premier keyword pour chercher
    const searchText = `${extractedTopic.title} ${extractedTopic.keywords.slice(0, 3).join(" ")}`
    const embedding = await generateEmbedding(searchText)

    const { data: semanticMatches } = await supabase.rpc(
      "match_topic_memories_by_keywords",
      {
        target_user_id: userId,
        query_embedding: embedding,
        match_threshold: 0.78, // High threshold for matching existing topics
        match_count: 1,
      } as any,
    )

    if (Array.isArray(semanticMatches) && semanticMatches.length > 0) {
      const match = semanticMatches[0]
      // Charger le topic complet
      const { data: fullTopic } = await supabase
        .from("user_topic_memories")
        .select("*")
        .eq("id", match.topic_id)
        .maybeSingle()

      if (fullTopic) return fullTopic as TopicMemory
    }
  }

  return null
}

// ============================================================================
// 3. ENRICHISSEMENT ‚Äî Mettre √† jour la synth√®se d'un topic existant
// ============================================================================

/**
 * Enrichit la synth√®se d'un topic existant avec de nouvelles informations.
 * Le LLM d√©cide si les nouvelles infos apportent quelque chose de nouveau.
 */
export async function enrichTopicSynthesis(opts: {
  supabase: SupabaseClient
  userId: string
  topic: TopicMemory
  newInformation: string
  newKeywords: string[]
  sourceType?: TopicEnrichmentSource
  meta?: { requestId?: string; model?: string; forceRealAi?: boolean }
}): Promise<{ enriched: boolean; newSynthesis?: string }> {
  const { supabase, userId, topic, newInformation, newKeywords, meta } = opts
  const sourceType = opts.sourceType ?? "chat"

  const prompt = `
Tu es le gestionnaire de m√©moire d'un coach IA.
Tu dois d√©cider si de nouvelles informations enrichissent un topic existant.

TOPIC EXISTANT :
- Titre : "${topic.title}"
- Synth√®se actuelle :
"${topic.synthesis}"

NOUVELLES INFORMATIONS :
"${newInformation}"

TES R√àGLES :
1. Si les nouvelles infos sont un doublon ou n'apportent RIEN de nouveau ‚Üí { "enriched": false }
2. Si les nouvelles infos enrichissent le topic ‚Üí produis une NOUVELLE SYNTH√àSE qui :
   - Int√®gre les nouvelles infos DANS la synth√®se existante (pas juste concat√©ner)
   - Maintient une progression chronologique naturelle
   - Garde les informations importantes du pass√©
   - Supprime les redondances
   - Reste dense et factuel (max 5 paragraphes courts)
   - Est √©crite √† la 3√®me personne ("Il/Elle...")
3. Si une info CONTREDIT une info pr√©c√©dente, mets √† jour (ex: "Il a repris le cannabis" remplace "Il a arr√™t√©")

JSON ATTENDU :
{ "enriched": true, "new_synthesis": "..." }
ou
{ "enriched": false }
  `.trim()

  try {
    const raw = await generateWithGemini(prompt, "", 0.1, true, [], "json", {
      requestId: meta?.requestId,
      model: meta?.model ?? "gemini-2.5-flash",
      source: "sophia-brain:topic_enrichment",
      forceRealAi: meta?.forceRealAi,
    })

    const result = JSON.parse(String(raw ?? "{}"))

    if (!result.enriched) {
      // Pas d'enrichissement, mais on incr√©mente le mention_count
      await supabase
        .from("user_topic_memories")
        .update({
          mention_count: (topic.mention_count ?? 0) + 1,
          updated_at: new Date().toISOString(),
        })
        .eq("id", topic.id)

      return { enriched: false }
    }

    const newSynthesis = String(result.new_synthesis ?? "").trim()
    if (!newSynthesis) return { enriched: false }

    // Mettre √† jour le topic
    const synthesisEmbedding = await generateEmbedding(newSynthesis)
    const now = new Date().toISOString()

    // Log l'enrichissement (audit trail)
    await supabase.from("user_topic_enrichment_log").insert({
      user_id: userId,
      topic_id: topic.id,
      enrichment_summary: newInformation.slice(0, 500),
      previous_synthesis: topic.synthesis,
      source_type: sourceType,
    })

    // Mettre √† jour le topic
    await supabase
      .from("user_topic_memories")
      .update({
        synthesis: newSynthesis,
        synthesis_embedding: synthesisEmbedding,
        mention_count: (topic.mention_count ?? 0) + 1,
        enrichment_count: (topic.enrichment_count ?? 0) + 1,
        last_enriched_at: now,
        updated_at: now,
      })
      .eq("id", topic.id)

    // Ajouter les nouveaux keywords
    await upsertKeywords({
      supabase,
      userId,
      topicId: topic.id,
      keywords: newKeywords,
    })

    console.log(`[TopicMemory] Enriched topic "${topic.title}" (id=${topic.id})`)
    return { enriched: true, newSynthesis }
  } catch (e) {
    console.error(`[TopicMemory] Failed to enrich topic "${topic.title}":`, e)
    return { enriched: false }
  }
}

// ============================================================================
// 4. CR√âATION ‚Äî Cr√©er un nouveau topic
// ============================================================================

/**
 * Cr√©e un nouveau topic √† partir d'informations extraites.
 */
export async function createTopic(opts: {
  supabase: SupabaseClient
  userId: string
  extractedTopic: ExtractedTopic
  sourceType?: TopicEnrichmentSource
  meta?: { requestId?: string; forceRealAi?: boolean }
}): Promise<TopicMemory | null> {
  const { supabase, userId, extractedTopic, meta } = opts
  const sourceType = opts.sourceType ?? "chat"

  // G√©n√©rer la synth√®se initiale (reformulation √† la 3√®me personne)
  const prompt = `
Reformule les informations suivantes en une synth√®se √† la 3√®me personne.
Sois dense, factuel, et organise par ordre chronologique si applicable.
1-2 paragraphes maximum. Commence directement par le contenu.

Informations : "${extractedTopic.new_information}"
Sujet : "${extractedTopic.title}"
  `.trim()

  let synthesis: string
  try {
    const raw = await generateWithGemini(prompt, "", 0.1, true, [], "auto", {
      model: "gemini-2.5-flash",
      source: "sophia-brain:topic_initial_synthesis",
      forceRealAi: meta?.forceRealAi,
    })
    synthesis = String(raw ?? extractedTopic.new_information).trim()
  } catch {
    synthesis = extractedTopic.new_information
  }

  // Vectoriser la synth√®se
  const synthesisEmbedding = await generateEmbedding(synthesis)
  const now = new Date().toISOString()

  const { data: newTopic, error } = await supabase
    .from("user_topic_memories")
    .insert({
      user_id: userId,
      slug: extractedTopic.slug,
      title: extractedTopic.title,
      synthesis,
      synthesis_embedding: synthesisEmbedding,
      status: "active",
      mention_count: 1,
      enrichment_count: 0,
      first_mentioned_at: now,
      last_enriched_at: now,
      metadata: {
        domain: extractedTopic.domain ?? null,
        source_type: sourceType,
      },
    })
    .select("*")
    .single()

  if (error) {
    console.error(`[TopicMemory] Failed to create topic "${extractedTopic.title}":`, error)
    return null
  }

  // Ajouter les keywords
  await upsertKeywords({
    supabase,
    userId,
    topicId: newTopic.id,
    keywords: extractedTopic.keywords,
  })

  console.log(`[TopicMemory] Created topic "${extractedTopic.title}" with ${extractedTopic.keywords.length} keywords`)
  return newTopic as TopicMemory
}

// ============================================================================
// 5. KEYWORDS ‚Äî Gestion des mots-cl√©s vectoris√©s
// ============================================================================

/**
 * Ajoute ou met √† jour des keywords pour un topic.
 * Si un keyword existe d√©j√† pour un AUTRE topic, il est r√©affect√©.
 */
async function upsertKeywords(opts: {
  supabase: SupabaseClient
  userId: string
  topicId: string
  keywords: string[]
}): Promise<void> {
  const { supabase, userId, topicId, keywords } = opts

  const uniqueKeywords = [...new Set(keywords.map(k => k.trim().toLowerCase()).filter(Boolean))]

  for (const keyword of uniqueKeywords) {
    try {
      const embedding = await generateEmbedding(keyword)

      // Upsert : si le keyword existe d√©j√†, on le r√©affecte √† ce topic
      await supabase
        .from("user_topic_keywords")
        .upsert(
          {
            user_id: userId,
            topic_id: topicId,
            keyword,
            keyword_embedding: embedding,
            source: "llm_extracted",
          },
          { onConflict: "user_id,keyword" },
        )
    } catch (e) {
      console.warn(`[TopicMemory] Failed to upsert keyword "${keyword}":`, e)
    }
  }
}

// ============================================================================
// 6. RETRIEVAL ‚Äî Recherche de topics pertinents pour le contexte
// ============================================================================

/**
 * Recherche les topics pertinents pour un message utilisateur.
 * Combine la recherche par keywords ET par synth√®se pour maximiser le recall.
 */
export async function retrieveTopicMemories(opts: {
  supabase: SupabaseClient
  userId: string
  message: string
  maxResults?: number
  meta?: { requestId?: string; forceRealAi?: boolean }
}): Promise<TopicSearchResult[]> {
  const { supabase, userId, message, maxResults = 3 } = opts

  const embedding = await generateEmbedding(message)

  // Recherche parall√®le : par keywords ET par synth√®se
  const [keywordResults, synthesisResults] = await Promise.all([
    supabase.rpc("match_topic_memories_by_keywords", {
      target_user_id: userId,
      query_embedding: embedding,
      match_threshold: 0.55, // Lower threshold for retrieval (more permissive)
      match_count: maxResults + 2,
    } as any).then((r: any) => (Array.isArray(r.data) ? r.data : []) as TopicSearchResult[]),

    supabase.rpc("match_topic_memories_by_synthesis", {
      target_user_id: userId,
      query_embedding: embedding,
      match_threshold: 0.50,
      match_count: maxResults,
    } as any).then((r: any) => (Array.isArray(r.data) ? r.data : []) as TopicSearchResult[]),
  ])

  // D√©dupliquer et fusionner les r√©sultats
  const seenIds = new Set<string>()
  const merged: TopicSearchResult[] = []

  // Priorit√© aux keyword matches (plus pr√©cis)
  for (const r of keywordResults) {
    if (!seenIds.has(r.topic_id)) {
      seenIds.add(r.topic_id)
      merged.push(r)
    }
  }

  // Ajouter les synthesis matches manquants
  for (const r of synthesisResults) {
    if (!seenIds.has(r.topic_id)) {
      seenIds.add(r.topic_id)
      merged.push(r)
    }
  }

  // Mettre √† jour last_retrieved_at pour les topics retourn√©s
  const topicIds = merged.slice(0, maxResults).map(r => r.topic_id)
  if (topicIds.length > 0) {
    try {
      await supabase
        .from("user_topic_memories")
        .update({ last_retrieved_at: new Date().toISOString() })
        .in("id", topicIds)
    } catch {
      // non-blocking
    }
  }

  return merged.slice(0, maxResults)
}

/**
 * Formate les topic memories pour injection dans le prompt du Companion.
 */
export function formatTopicMemoriesForPrompt(topics: TopicSearchResult[]): string {
  if (!topics || topics.length === 0) return ""

  let block = "=== M√âMOIRE TH√âMATIQUE (CE QUE TU SAIS DE LUI/ELLE) ===\n"

  for (const topic of topics) {
    const enrichedAt = topic.last_enriched_at
      ? new Date(topic.last_enriched_at).toLocaleDateString("fr-FR")
      : "inconnue"
    const mentions = topic.mention_count ?? 0

    block += `\nüìå ${topic.title} (mentionn√© ${mentions}x, derni√®re m√†j: ${enrichedAt})\n`
    block += `${topic.synthesis}\n`
  }

  block += "\n- Utilise ces informations NATURELLEMENT, sans les exposer.\n"
  block += "- Ne dis pas \"je sais que...\" ou \"dans ta m√©moire...\". Juste utilise.\n"
  block += "- Si un topic est pertinent, int√®gre-le subtilement dans ta r√©ponse.\n\n"

  return block
}

// ============================================================================
// 7. PIPELINE ‚Äî Orchestration compl√®te (appel√© par le Watcher)
// ============================================================================

/**
 * Pipeline complet de traitement des topics apr√®s analyse d'un batch.
 * Appel√© par le Watcher apr√®s l'extraction.
 */
export async function processTopicsFromWatcher(opts: {
  supabase: SupabaseClient
  userId: string
  transcript: string
  currentContext?: string
  sourceType?: TopicEnrichmentSource
  meta?: { requestId?: string; model?: string; forceRealAi?: boolean }
}): Promise<{ topicsCreated: number; topicsEnriched: number }> {
  const { supabase, userId, transcript, currentContext, meta } = opts
  const sourceType = opts.sourceType ?? "chat"

  // 1. Charger les slugs existants pour le LLM
  const { data: existingTopics } = await supabase
    .from("user_topic_memories")
    .select("slug")
    .eq("user_id", userId)
    .eq("status", "active")
    .limit(50)

  const existingTopicSlugs = (existingTopics ?? []).map((t: any) => String(t.slug))

  // 2. Extraire les topics de la conversation
  const extractedTopics = await extractTopicsFromTranscript({
    transcript,
    existingTopicSlugs,
    currentContext,
    meta,
  })

  if (extractedTopics.length === 0) {
    console.log("[TopicMemory] No topics extracted from transcript.")
    return { topicsCreated: 0, topicsEnriched: 0 }
  }

  console.log(`[TopicMemory] Extracted ${extractedTopics.length} topics: ${extractedTopics.map(t => t.slug).join(", ")}`)

  let topicsCreated = 0
  let topicsEnriched = 0

  // 3. Pour chaque topic : enrichir ou cr√©er
  for (const extracted of extractedTopics) {
    try {
      const existingTopic = await findMatchingTopic({
        supabase,
        userId,
        extractedTopic: extracted,
      })

      if (existingTopic) {
        // Enrichir le topic existant
        const result = await enrichTopicSynthesis({
          supabase,
          userId,
          topic: existingTopic,
          newInformation: extracted.new_information,
          newKeywords: extracted.keywords,
          sourceType,
          meta,
        })
        if (result.enriched) topicsEnriched++
      } else {
        // Cr√©er un nouveau topic
        const created = await createTopic({
          supabase,
          userId,
          extractedTopic: extracted,
          sourceType,
          meta,
        })
        if (created) topicsCreated++
      }
    } catch (e) {
      console.error(`[TopicMemory] Failed to process topic "${extracted.slug}":`, e)
    }
  }

  console.log(`[TopicMemory] Pipeline done: ${topicsCreated} created, ${topicsEnriched} enriched.`)
  return { topicsCreated, topicsEnriched }
}

/**
 * Ingestion cibl√©e des topics √† partir des inputs utilisateur d'un plan.
 * Utilise uniquement les champs user-authored stock√©s dans user_plans.
 */
export async function processTopicsFromPlan(opts: {
  supabase: SupabaseClient
  userId: string
  plan: {
    id?: string
    title?: string | null
    inputs_why?: string | null
    inputs_blockers?: string | null
    inputs_context?: string | null
    recraft_reason?: string | null
    recraft_challenges?: string | null
  }
  meta?: { requestId?: string; model?: string; forceRealAi?: boolean }
}): Promise<{ topicsCreated: number; topicsEnriched: number }> {
  const { supabase, userId, plan, meta } = opts

  const rows: string[] = []
  const pushIfPresent = (label: string, value?: string | null) => {
    const text = String(value ?? "").trim()
    if (text.length > 0) rows.push(`USER: ${label}: ${text}`)
  }

  pushIfPresent("Mon pourquoi", plan.inputs_why)
  pushIfPresent("Mes blocages", plan.inputs_blockers)
  pushIfPresent("Mon contexte", plan.inputs_context)
  pushIfPresent("Raison du recraft", plan.recraft_reason)
  pushIfPresent("Difficult√©s du recraft", plan.recraft_challenges)

  if (rows.length === 0) {
    return { topicsCreated: 0, topicsEnriched: 0 }
  }

  const transcript = rows.join("\n")
  const currentContext = `Extraction depuis plan${plan.title ? `: ${String(plan.title)}` : ""}${plan.id ? ` (id=${String(plan.id)})` : ""}`

  return await processTopicsFromWatcher({
    supabase,
    userId,
    transcript,
    currentContext,
    sourceType: "plan",
    meta,
  })
}

// ============================================================================
// Helpers
// ============================================================================

/** Normalise un slug (lowercase, underscores, pas de caract√®res sp√©ciaux) */
function slugify(text: string): string {
  return text
    .toLowerCase()
    .trim()
    .normalize("NFD")
    .replace(/[\u0300-\u036f]/g, "") // Remove accents
    .replace(/[^a-z0-9_]/g, "_")     // Replace non-alphanumeric with _
    .replace(/_+/g, "_")             // Collapse multiple _
    .replace(/^_|_$/g, "")           // Trim leading/trailing _
    .slice(0, 80)                    // Max length
}
